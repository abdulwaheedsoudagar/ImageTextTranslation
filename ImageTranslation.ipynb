{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageTranslation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0wJB8p-JyKp"
      },
      "source": [
        "!pip install pyyaml==5.1\r\n",
        "import torch, torchvision\r\n",
        "print(torch.__version__, torch.cuda.is_available())\r\n",
        "!gcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwIiZmokJ1CR"
      },
      "source": [
        "import torch\r\n",
        "assert torch.__version__.startswith(\"1.7\")\r\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\r\n",
        "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxfpdhOqsG4B"
      },
      "source": [
        "import os\r\n",
        "os.chdir('/content/drive/MyDrive/text-detection/')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyEUSIThJ1YE"
      },
      "source": [
        "import detectron2\r\n",
        "from detectron2.utils.logger import setup_logger\r\n",
        "setup_logger()\r\n",
        "\r\n",
        "# import some common libraries\r\n",
        "import numpy as np\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "\r\n",
        "from google.colab.patches import cv2_imshow\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "# import some common detectron2 utilities\r\n",
        "from detectron2 import model_zoo\r\n",
        "from detectron2.engine import DefaultPredictor\r\n",
        "from detectron2.config import get_cfg\r\n",
        "from detectron2.utils.visualizer import Visualizer\r\n",
        "from detectron2.data import MetadataCatalog\r\n",
        "from detectron2.config import get_cfg\r\n",
        "from detectron2.utils.visualizer import ColorMode\r\n",
        "\r\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfFLFIrxF1vh"
      },
      "source": [
        "import cv2\r\n",
        "import numpy as np\r\n",
        "import string\r\n",
        "import time\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "import tensorflow.keras.backend as K\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.utils import multi_gpu_model\r\n",
        "from tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\r\n",
        "\r\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM, CuDNNGRU\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.activations import relu, sigmoid, softmax\r\n",
        "\r\n",
        "from tensorflow.keras.utils import to_categorical, Sequence\r\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from tensorflow.keras.activations import elu\r\n",
        "from tqdm import tqdm\r\n",
        "from collections import Counter\r\n",
        "\r\n",
        "from PIL import Image, ImageFont, ImageDraw, ImageFilter\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch.nn.functional as F\r\n",
        "from IPython.display import clear_output\r\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJbKOwlOKDJ9"
      },
      "source": [
        "act_model,char_list,_,_=image_text_model()\r\n",
        "act_model.load_weights('models/image_to_text.hdf5')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhbAkrE4un84",
        "outputId": "9a9d104b-891b-4be3-9f84-40db4c0ab15b"
      },
      "source": [
        "MAX_OUTPUT_CHARS = 30\r\n",
        "eng_alphabets = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\r\n",
        "pad_char = '-PAD-'\r\n",
        "\r\n",
        "eng_alpha2index = {pad_char: 0}\r\n",
        "for index, alpha in enumerate(eng_alphabets):\r\n",
        "    eng_alpha2index[alpha] = index+1\r\n",
        "\r\n",
        "kannada_alphabets = [chr(alpha) for alpha in range(3202, 3311)]\r\n",
        "kannada_alphabet_size = len(kannada_alphabets)\r\n",
        "\r\n",
        "kannada_alpha2index = {pad_char: 0}\r\n",
        "for index, alpha in enumerate(kannada_alphabets):\r\n",
        "    kannada_alpha2index[alpha] = index+1\r\n",
        "\r\n",
        "translation_model = Transliteration_EncoderDecoder_Attention(len(eng_alpha2index), 256, len(kannada_alpha2index), verbose=True)\r\n",
        "translation_model=torch.load('models/translation.pt')\r\n",
        "translation_model.eval()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transliteration_EncoderDecoder_Attention(\n",
              "  (encoder_rnn_cell): GRU(27, 256)\n",
              "  (encoder_rnn_cell2): GRU(256, 256)\n",
              "  (decoder_rnn_cell): GRU(512, 256)\n",
              "  (decoder_rnn_cell2): GRU(512, 256)\n",
              "  (h2o): Linear(in_features=256, out_features=110, bias=True)\n",
              "  (softmax): LogSoftmax(dim=2)\n",
              "  (U): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (W): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (attn): Linear(in_features=256, out_features=1, bias=True)\n",
              "  (out2hidden): Linear(in_features=110, out_features=256, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQc6GPgMSkfp"
      },
      "source": [
        "cfg = get_cfg()\r\n",
        "cfg.merge_from_file(model_zoo.get_config_file('COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml'))\r\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6 # Set threshold for this model\r\n",
        "cfg.MODEL.WEIGHTS = \"models/model_final.pth\"\r\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\r\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HQQ5fWHcELk"
      },
      "source": [
        "def complete_prediction(img_path,font_size=60):\r\n",
        "  test_image = cv2.imread(img_path, cv2.COLOR_BGR2RGB)\r\n",
        "  outputs = predictor(test_image)\r\n",
        "  font_size=font_size\r\n",
        "  bounding_box=outputs['instances'][outputs['instances'].pred_classes==1].pred_boxes.tensor.cpu().numpy()\r\n",
        "  test_image=Image.open(os.path.join(img_path))\r\n",
        "  final_img = cv2.imread(img_path, cv2.COLOR_BGR2RGB)\r\n",
        "  final_img = Image.fromarray(final_img)\r\n",
        "  i=0\r\n",
        "  for box in bounding_box:\r\n",
        "      crop_img=test_image.crop(box)\r\n",
        "      img_path='croppedimages/'+'img'+str(i)+'.jpg'\r\n",
        "      crop_img.save(img_path)\r\n",
        "      crop_img=pre_process_image(img_path)\r\n",
        "      test_word=predict_output(crop_img,act_model,char_list)\r\n",
        "      test_word=test_word.upper()\r\n",
        "      translated_word=language_translation(translation_model, test_word, eng_alpha2index, device = 'cpu')\r\n",
        "      ImageDraw.Draw(final_img).text((box[0], box[1]-70), translated_word, font=unicode_font, fill=(b,g,r))\r\n",
        "      i=i+1\r\n",
        "  return final_img"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czb6twB-upH0"
      },
      "source": [
        "unicode_font = ImageFont.truetype(\"./akshar.ttf\", font_size)\r\n",
        "b,g,r = 0,0,0"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K3a771ccHAs"
      },
      "source": [
        "plt.figure(figsize=(10,10))\r\n",
        "plt.axis(\"off\")\r\n",
        "img_path='/content/new4.jpg'\r\n",
        "output_img=complete_prediction(img_path,80)\r\n",
        "plt.imshow(output_img);"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVGbKRJjdOEu"
      },
      "source": [
        ""
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzfH5XerLgHN"
      },
      "source": [
        ""
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZuyLQI4ZWYo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}